{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rAEh66-hUCIK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "data = pd.read_csv('US_nn_plus.csv')\n",
        "profit_mean = data['profit'].mean()\n",
        "profit_std = data['profit'].std()\n",
        "#Delete column profit\n",
        "data = data.drop(columns=['profit'])\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.input = torch.tensor(data[data.columns[5:]].values, dtype=torch.float32)\n",
        "        self.output = torch.tensor(data[data.columns[0:5]].values, dtype=torch.float32)\n",
        "    def __getitem__(self, index):\n",
        "        x = self.input[index]\n",
        "        y_recom = self.output[index, :]\n",
        "        return x, y_recom\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "dataset = CustomDataset(data)\n",
        "train, val = torch.utils.data.random_split(dataset, [0.8,0.2])\n",
        "train_loader = DataLoader(train, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val, batch_size=len(val), shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWGbdXSDUCIK",
        "outputId": "ac17d109-2239-418d-cd6e-777771031a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12159\n"
          ]
        }
      ],
      "source": [
        "class res_block(nn.Module):\n",
        "  def __init__(self, in_features, out_features,dropout=0.5):\n",
        "      super(res_block, self).__init__()\n",
        "      self.fc1 = nn.Linear(in_features, out_features)\n",
        "      self.bn1 = nn.BatchNorm1d(out_features)\n",
        "      self.dropout1 = nn.Dropout(dropout)\n",
        "      self.fc2 = nn.Linear(out_features, out_features)\n",
        "      self.bn2 = nn.BatchNorm1d(out_features)\n",
        "      self.dropout2 = nn.Dropout(dropout)\n",
        "      self.fc3 = nn.Linear(in_features, out_features)\n",
        "  def forward(self, x):\n",
        "      residual = x.clone()\n",
        "      x = F.relu(self.dropout1(self.bn1(self.fc1(x))))\n",
        "      x = self.dropout2(self.bn2(self.fc2(x)))\n",
        "      x = x + self.fc3(residual)\n",
        "      return F.relu(x)\n",
        "\n",
        "class simple_model(nn.Module):\n",
        "  def __init__(self,drop=0.3):\n",
        "    super(simple_model, self).__init__()\n",
        "    self.bn_start = nn.BatchNorm1d(13)\n",
        "    self.block1 = res_block(13, 32, drop)\n",
        "    self.block2 = res_block(32,64,drop)\n",
        "    self.block3 = res_block(64,128,drop)\n",
        "    self.block4 = res_block(128,64,drop)\n",
        "    self.block5 = res_block(64,32,drop)\n",
        "    self.block6 = res_block(32,5,drop)\n",
        "  def forward(self, x):\n",
        "    x = self.bn_start(x)\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.block5(x)\n",
        "    x = self.block6(x)\n",
        "    return x\n",
        "\n",
        "class small_model(nn.Module):\n",
        "  def __init__(self,drop=0.3):\n",
        "    super(small_model, self).__init__()\n",
        "    self.bn_start = nn.BatchNorm1d(13)\n",
        "    self.block1 = res_block(13, 32, drop)\n",
        "    self.block2 = res_block(32,32,drop)\n",
        "    self.block3 = res_block(32,32,drop)\n",
        "    self.block4 = res_block(32,32,drop)\n",
        "    self.fn = nn.Linear(32,5)\n",
        "  def forward(self, x):\n",
        "    x = self.bn_start(x)\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.block4(x)\n",
        "    x = self.fn(x)\n",
        "    return x\n",
        "\n",
        "def loss_fn(x, y_recom):\n",
        "    recom = F.softmax(x, dim=1)\n",
        "    return F.binary_cross_entropy(recom, y_recom)\n",
        "\n",
        "model = small_model(0.2).to('cuda')\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "writer = SummaryWriter()\n",
        "exmaple_batch = next(iter(train_loader))\n",
        "x, y = exmaple_batch\n",
        "writer.add_graph(model, x.to('cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1B_ko9JucGWu"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bUFjkYR8UCIK"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=0.001)\n",
        "#Use simple scheduler\n",
        "sceduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "def train_one_epoch(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "    for x, y_recom in train_loader:\n",
        "        x = x.to('cuda')\n",
        "        y_recom = y_recom.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = loss_fn(output,  y_recom)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "    return loss.item()/len(train_loader)\n",
        "def val_loss(model, val):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y_recom in val:\n",
        "            x = x.to('cuda')\n",
        "            y_recom = y_recom.to('cuda')\n",
        "            output = model(x)\n",
        "            loss = loss_fn(output, y_recom)\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ogo5Hx8qUCIL",
        "outputId": "e8a35512-918d-49ff-9a85-14b963f0f798"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 300\n",
            "Train loss: 0.036402422934770584\n",
            "Val loss: 0.42413660883903503\n",
            "Epoch: 301\n",
            "Train loss: 0.03671134263277054\n",
            "Val loss: 0.4154580235481262\n",
            "Epoch: 302\n",
            "Train loss: 0.04266427084803581\n",
            "Val loss: 0.419908344745636\n",
            "Epoch: 303\n",
            "Train loss: 0.03509564325213432\n",
            "Val loss: 0.41586148738861084\n",
            "Epoch: 304\n",
            "Train loss: 0.03931846842169762\n",
            "Val loss: 0.4156135022640228\n",
            "Epoch: 305\n",
            "Train loss: 0.03549457713961601\n",
            "Val loss: 0.43117016553878784\n",
            "Epoch: 306\n",
            "Train loss: 0.029728049412369728\n",
            "Val loss: 0.44474926590919495\n",
            "Epoch: 307\n",
            "Train loss: 0.04399974271655083\n",
            "Val loss: 0.42615726590156555\n",
            "Epoch: 308\n",
            "Train loss: 0.026687443256378174\n",
            "Val loss: 0.4371587336063385\n",
            "Epoch: 309\n",
            "Train loss: 0.017793603241443634\n",
            "Val loss: 0.4417608380317688\n",
            "Epoch: 310\n",
            "Train loss: 0.0360056534409523\n",
            "Val loss: 0.44482433795928955\n",
            "Epoch: 311\n",
            "Train loss: 0.03838464617729187\n",
            "Val loss: 0.43446874618530273\n",
            "Epoch: 312\n",
            "Train loss: 0.030972803011536598\n",
            "Val loss: 0.4360280930995941\n",
            "Epoch: 313\n",
            "Train loss: 0.029922345653176308\n",
            "Val loss: 0.4311678111553192\n",
            "Epoch: 314\n",
            "Train loss: 0.02866743877530098\n",
            "Val loss: 0.44715002179145813\n",
            "Epoch: 315\n",
            "Train loss: 0.03305767849087715\n",
            "Val loss: 0.43882542848587036\n",
            "Epoch: 316\n",
            "Train loss: 0.0255944412201643\n",
            "Val loss: 0.4320237934589386\n",
            "Epoch: 317\n",
            "Train loss: 0.034220438450574875\n",
            "Val loss: 0.4219962954521179\n",
            "Epoch: 318\n",
            "Train loss: 0.02486957050859928\n",
            "Val loss: 0.4346242845058441\n",
            "Epoch: 319\n",
            "Train loss: 0.041249342262744904\n",
            "Val loss: 0.4406077563762665\n",
            "Epoch: 320\n",
            "Train loss: 0.03143258020281792\n",
            "Val loss: 0.451545774936676\n",
            "Epoch: 321\n",
            "Train loss: 0.028545072302222252\n",
            "Val loss: 0.43982240557670593\n",
            "Epoch: 322\n",
            "Train loss: 0.0248174536973238\n",
            "Val loss: 0.43347591161727905\n",
            "Epoch: 323\n",
            "Train loss: 0.045231178402900696\n",
            "Val loss: 0.43444204330444336\n",
            "Epoch: 324\n",
            "Train loss: 0.03856778144836426\n",
            "Val loss: 0.43423929810523987\n",
            "Epoch: 325\n",
            "Train loss: 0.03549392521381378\n",
            "Val loss: 0.4321243464946747\n",
            "Epoch: 326\n",
            "Train loss: 0.028225097805261612\n",
            "Val loss: 0.4377388060092926\n",
            "Epoch: 327\n",
            "Train loss: 0.035240672528743744\n",
            "Val loss: 0.4535941779613495\n",
            "Epoch: 328\n",
            "Train loss: 0.026158655062317848\n",
            "Val loss: 0.45131343603134155\n",
            "Epoch: 329\n",
            "Train loss: 0.036409515887498856\n",
            "Val loss: 0.44801607728004456\n",
            "Epoch: 330\n",
            "Train loss: 0.026120079681277275\n",
            "Val loss: 0.4438683092594147\n",
            "Epoch: 331\n",
            "Train loss: 0.025271547958254814\n",
            "Val loss: 0.43425920605659485\n",
            "Epoch: 332\n",
            "Train loss: 0.023381656035780907\n",
            "Val loss: 0.44873180985450745\n",
            "Epoch: 333\n",
            "Train loss: 0.05095192417502403\n",
            "Val loss: 0.4480527937412262\n",
            "Epoch: 334\n",
            "Train loss: 0.01994912326335907\n",
            "Val loss: 0.4433378577232361\n",
            "Epoch: 335\n",
            "Train loss: 0.029627591371536255\n",
            "Val loss: 0.460056871175766\n",
            "Epoch: 336\n",
            "Train loss: 0.023290419951081276\n",
            "Val loss: 0.4593694806098938\n",
            "Epoch: 337\n",
            "Train loss: 0.030532745644450188\n",
            "Val loss: 0.466383159160614\n",
            "Epoch: 338\n",
            "Train loss: 0.02789868228137493\n",
            "Val loss: 0.4857691526412964\n",
            "Epoch: 339\n",
            "Train loss: 0.026739168912172318\n",
            "Val loss: 0.46697720885276794\n",
            "Epoch: 340\n",
            "Train loss: 0.014407767914235592\n",
            "Val loss: 0.47105371952056885\n",
            "Epoch: 341\n",
            "Train loss: 0.016137709841132164\n",
            "Val loss: 0.4810856282711029\n",
            "Epoch: 342\n",
            "Train loss: 0.013385904021561146\n",
            "Val loss: 0.46867361664772034\n",
            "Epoch: 343\n",
            "Train loss: 0.022499943152070045\n",
            "Val loss: 0.46190348267555237\n",
            "Epoch: 344\n",
            "Train loss: 0.032352518290281296\n",
            "Val loss: 0.46402478218078613\n",
            "Epoch: 345\n",
            "Train loss: 0.02787327580153942\n",
            "Val loss: 0.47120022773742676\n",
            "Epoch: 346\n",
            "Train loss: 0.03370441868901253\n",
            "Val loss: 0.4768063426017761\n",
            "Epoch: 347\n",
            "Train loss: 0.025217195972800255\n",
            "Val loss: 0.46779516339302063\n",
            "Epoch: 348\n",
            "Train loss: 0.02139933779835701\n",
            "Val loss: 0.4696506857872009\n",
            "Epoch: 349\n",
            "Train loss: 0.02506878413259983\n",
            "Val loss: 0.47864624857902527\n",
            "Epoch: 350\n",
            "Train loss: 0.02706209197640419\n",
            "Val loss: 0.503222644329071\n",
            "Epoch: 351\n",
            "Train loss: 0.01764415018260479\n",
            "Val loss: 0.48260337114334106\n",
            "Epoch: 352\n",
            "Train loss: 0.029861992225050926\n",
            "Val loss: 0.4571717381477356\n",
            "Epoch: 353\n",
            "Train loss: 0.035054512321949005\n",
            "Val loss: 0.47681793570518494\n",
            "Epoch: 354\n",
            "Train loss: 0.02353345975279808\n",
            "Val loss: 0.48285171389579773\n",
            "Epoch: 355\n",
            "Train loss: 0.02887207828462124\n",
            "Val loss: 0.4806866943836212\n",
            "Epoch: 356\n",
            "Train loss: 0.035270530730485916\n",
            "Val loss: 0.46019136905670166\n",
            "Epoch: 357\n",
            "Train loss: 0.03763442113995552\n",
            "Val loss: 0.44988003373146057\n",
            "Epoch: 358\n",
            "Train loss: 0.016458258032798767\n",
            "Val loss: 0.450592964887619\n",
            "Epoch: 359\n",
            "Train loss: 0.030602825805544853\n",
            "Val loss: 0.4683893024921417\n",
            "Epoch: 360\n",
            "Train loss: 0.019616344943642616\n",
            "Val loss: 0.4652511775493622\n",
            "Epoch: 361\n",
            "Train loss: 0.012290644459426403\n",
            "Val loss: 0.457210510969162\n",
            "Epoch: 362\n",
            "Train loss: 0.029213134199380875\n",
            "Val loss: 0.47450053691864014\n",
            "Epoch: 363\n",
            "Train loss: 0.02244756743311882\n",
            "Val loss: 0.47479698061943054\n",
            "Epoch: 364\n",
            "Train loss: 0.03548287972807884\n",
            "Val loss: 0.4618946313858032\n",
            "Epoch: 365\n",
            "Train loss: 0.03559532389044762\n",
            "Val loss: 0.4675518870353699\n",
            "Epoch: 366\n",
            "Train loss: 0.03806665912270546\n",
            "Val loss: 0.4808644652366638\n",
            "Epoch: 367\n",
            "Train loss: 0.021818816661834717\n",
            "Val loss: 0.4927433431148529\n",
            "Epoch: 368\n",
            "Train loss: 0.03420056030154228\n",
            "Val loss: 0.4933859407901764\n",
            "Epoch: 369\n",
            "Train loss: 0.019054219126701355\n",
            "Val loss: 0.5010023713111877\n",
            "Epoch: 370\n",
            "Train loss: 0.01491719763725996\n",
            "Val loss: 0.47962984442710876\n",
            "Epoch: 371\n",
            "Train loss: 0.03214552626013756\n",
            "Val loss: 0.48455968499183655\n",
            "Epoch: 372\n",
            "Train loss: 0.014863735996186733\n",
            "Val loss: 0.4808622896671295\n",
            "Epoch: 373\n",
            "Train loss: 0.040034759789705276\n",
            "Val loss: 0.47182637453079224\n",
            "Epoch: 374\n",
            "Train loss: 0.023424087092280388\n",
            "Val loss: 0.4711460471153259\n",
            "Epoch: 375\n",
            "Train loss: 0.020435525104403496\n",
            "Val loss: 0.4537465572357178\n",
            "Epoch: 376\n",
            "Train loss: 0.02511872723698616\n",
            "Val loss: 0.4757883846759796\n",
            "Epoch: 377\n",
            "Train loss: 0.024938123300671577\n",
            "Val loss: 0.4797556698322296\n",
            "Epoch: 378\n",
            "Train loss: 0.025119632482528687\n",
            "Val loss: 0.490105003118515\n",
            "Epoch: 379\n",
            "Train loss: 0.027900440618395805\n",
            "Val loss: 0.4884278476238251\n",
            "Epoch: 380\n",
            "Train loss: 0.021029630675911903\n",
            "Val loss: 0.49309042096138\n",
            "Epoch: 381\n",
            "Train loss: 0.024362921714782715\n",
            "Val loss: 0.49416622519493103\n",
            "Epoch: 382\n",
            "Train loss: 0.021080322563648224\n",
            "Val loss: 0.5020301342010498\n",
            "Epoch: 383\n",
            "Train loss: 0.040582191199064255\n",
            "Val loss: 0.4969281554222107\n",
            "Epoch: 384\n",
            "Train loss: 0.02077590487897396\n",
            "Val loss: 0.48545539379119873\n",
            "Epoch: 385\n",
            "Train loss: 0.026363898068666458\n",
            "Val loss: 0.48614782094955444\n",
            "Epoch: 386\n",
            "Train loss: 0.03005729429423809\n",
            "Val loss: 0.4883432686328888\n",
            "Epoch: 387\n",
            "Train loss: 0.01324686873704195\n",
            "Val loss: 0.47602927684783936\n",
            "Epoch: 388\n",
            "Train loss: 0.024937966838479042\n",
            "Val loss: 0.4896567761898041\n",
            "Epoch: 389\n",
            "Train loss: 0.01615658961236477\n",
            "Val loss: 0.5009801387786865\n",
            "Epoch: 390\n",
            "Train loss: 0.027389584109187126\n",
            "Val loss: 0.48386892676353455\n",
            "Epoch: 391\n",
            "Train loss: 0.028710147365927696\n",
            "Val loss: 0.4876689910888672\n",
            "Epoch: 392\n",
            "Train loss: 0.017019229009747505\n",
            "Val loss: 0.49707743525505066\n",
            "Epoch: 393\n",
            "Train loss: 0.026312097907066345\n",
            "Val loss: 0.49499768018722534\n",
            "Epoch: 394\n",
            "Train loss: 0.023971833288669586\n",
            "Val loss: 0.5004222989082336\n",
            "Epoch: 395\n",
            "Train loss: 0.026564091444015503\n",
            "Val loss: 0.47709569334983826\n",
            "Epoch: 396\n",
            "Train loss: 0.03577657416462898\n",
            "Val loss: 0.4718957543373108\n",
            "Epoch: 397\n",
            "Train loss: 0.017787618562579155\n",
            "Val loss: 0.4751792550086975\n",
            "Epoch: 398\n",
            "Train loss: 0.013241506181657314\n",
            "Val loss: 0.47882264852523804\n",
            "Epoch: 399\n",
            "Train loss: 0.021656973287463188\n",
            "Val loss: 0.4826835095882416\n",
            "Epoch: 400\n",
            "Train loss: 0.028551161289215088\n",
            "Val loss: 0.4963817596435547\n",
            "Epoch: 401\n",
            "Train loss: 0.02140270359814167\n",
            "Val loss: 0.5080606937408447\n",
            "Epoch: 402\n",
            "Train loss: 0.027200639247894287\n",
            "Val loss: 0.5186811685562134\n",
            "Epoch: 403\n",
            "Train loss: 0.031846221536397934\n",
            "Val loss: 0.5051549673080444\n",
            "Epoch: 404\n",
            "Train loss: 0.02724742330610752\n",
            "Val loss: 0.4849643111228943\n",
            "Epoch: 405\n",
            "Train loss: 0.015436843037605286\n",
            "Val loss: 0.49859875440597534\n",
            "Epoch: 406\n",
            "Train loss: 0.05033319070935249\n",
            "Val loss: 0.49976930022239685\n",
            "Epoch: 407\n",
            "Train loss: 0.024061424657702446\n",
            "Val loss: 0.4872705638408661\n",
            "Epoch: 408\n",
            "Train loss: 0.024471288546919823\n",
            "Val loss: 0.489641010761261\n",
            "Epoch: 409\n",
            "Train loss: 0.02062252163887024\n",
            "Val loss: 0.5050049424171448\n",
            "Epoch: 410\n",
            "Train loss: 0.026614142581820488\n",
            "Val loss: 0.5204921364784241\n",
            "Epoch: 411\n",
            "Train loss: 0.02615763247013092\n",
            "Val loss: 0.5078364610671997\n",
            "Epoch: 412\n",
            "Train loss: 0.020239505916833878\n",
            "Val loss: 0.5188326239585876\n",
            "Epoch: 413\n",
            "Train loss: 0.02536037005484104\n",
            "Val loss: 0.4985591769218445\n",
            "Epoch: 414\n",
            "Train loss: 0.027031583711504936\n",
            "Val loss: 0.485655277967453\n",
            "Epoch: 415\n",
            "Train loss: 0.019987760111689568\n",
            "Val loss: 0.4779297113418579\n",
            "Epoch: 416\n",
            "Train loss: 0.030707601457834244\n",
            "Val loss: 0.4922005236148834\n",
            "Epoch: 417\n",
            "Train loss: 0.019237907603383064\n",
            "Val loss: 0.49931642413139343\n",
            "Epoch: 418\n",
            "Train loss: 0.03598255291581154\n",
            "Val loss: 0.5072113275527954\n",
            "Epoch: 419\n",
            "Train loss: 0.04373801499605179\n",
            "Val loss: 0.5209649205207825\n",
            "Epoch: 420\n",
            "Train loss: 0.02969813346862793\n",
            "Val loss: 0.5020030736923218\n",
            "Epoch: 421\n",
            "Train loss: 0.021394969895482063\n",
            "Val loss: 0.4903298020362854\n",
            "Epoch: 422\n",
            "Train loss: 0.023891745135188103\n",
            "Val loss: 0.4969054162502289\n",
            "Epoch: 423\n",
            "Train loss: 0.012785213999450207\n",
            "Val loss: 0.48375025391578674\n",
            "Epoch: 424\n",
            "Train loss: 0.011499865911900997\n",
            "Val loss: 0.49229228496551514\n",
            "Epoch: 425\n",
            "Train loss: 0.02883048728108406\n",
            "Val loss: 0.49983084201812744\n",
            "Epoch: 426\n",
            "Train loss: 0.03376581892371178\n",
            "Val loss: 0.495954304933548\n",
            "Epoch: 427\n",
            "Train loss: 0.011224368587136269\n",
            "Val loss: 0.511874794960022\n",
            "Epoch: 428\n",
            "Train loss: 0.021603919565677643\n",
            "Val loss: 0.5252095460891724\n",
            "Epoch: 429\n",
            "Train loss: 0.03475354611873627\n",
            "Val loss: 0.5427277684211731\n",
            "Epoch: 430\n",
            "Train loss: 0.02526874653995037\n",
            "Val loss: 0.541975736618042\n",
            "Epoch: 431\n",
            "Train loss: 0.03753375634551048\n",
            "Val loss: 0.5377296805381775\n",
            "Epoch: 432\n",
            "Train loss: 0.02803546003997326\n",
            "Val loss: 0.5451820492744446\n",
            "Epoch: 433\n",
            "Train loss: 0.021103838458657265\n",
            "Val loss: 0.5324257016181946\n",
            "Epoch: 434\n",
            "Train loss: 0.010487779043614864\n",
            "Val loss: 0.5222881436347961\n",
            "Epoch: 435\n",
            "Train loss: 0.022195691242814064\n",
            "Val loss: 0.5214146375656128\n",
            "Epoch: 436\n",
            "Train loss: 0.021146198734641075\n",
            "Val loss: 0.5235549211502075\n",
            "Epoch: 437\n",
            "Train loss: 0.020895544439554214\n",
            "Val loss: 0.5359634160995483\n",
            "Epoch: 438\n",
            "Train loss: 0.013504783622920513\n",
            "Val loss: 0.533748209476471\n",
            "Epoch: 439\n",
            "Train loss: 0.01680038869380951\n",
            "Val loss: 0.5366616249084473\n",
            "Epoch: 440\n",
            "Train loss: 0.04042545706033707\n",
            "Val loss: 0.5428029298782349\n",
            "Epoch: 441\n",
            "Train loss: 0.0199208315461874\n",
            "Val loss: 0.5403035879135132\n",
            "Epoch: 442\n",
            "Train loss: 0.02956821583211422\n",
            "Val loss: 0.5312462449073792\n",
            "Epoch: 443\n",
            "Train loss: 0.01814194582402706\n",
            "Val loss: 0.5435493588447571\n",
            "Epoch: 444\n",
            "Train loss: 0.027899449691176414\n",
            "Val loss: 0.5317158102989197\n",
            "Epoch: 445\n",
            "Train loss: 0.02036159858107567\n",
            "Val loss: 0.535324215888977\n",
            "Epoch: 446\n",
            "Train loss: 0.026971379294991493\n",
            "Val loss: 0.5354172587394714\n",
            "Epoch: 447\n",
            "Train loss: 0.02206149511039257\n",
            "Val loss: 0.5218065977096558\n",
            "Epoch: 448\n",
            "Train loss: 0.05225549265742302\n",
            "Val loss: 0.5064390897750854\n",
            "Epoch: 449\n",
            "Train loss: 0.03223658725619316\n",
            "Val loss: 0.5231266617774963\n",
            "Epoch: 450\n",
            "Train loss: 0.018342001363635063\n",
            "Val loss: 0.5198238492012024\n",
            "Epoch: 451\n",
            "Train loss: 0.03127870708703995\n",
            "Val loss: 0.5245615839958191\n",
            "Epoch: 452\n",
            "Train loss: 0.020201895385980606\n",
            "Val loss: 0.5204493403434753\n",
            "Epoch: 453\n",
            "Train loss: 0.02586432173848152\n",
            "Val loss: 0.5380740165710449\n",
            "Epoch: 454\n",
            "Train loss: 0.016549600288271904\n",
            "Val loss: 0.5565441250801086\n",
            "Epoch: 455\n",
            "Train loss: 0.01936892606317997\n",
            "Val loss: 0.5780784487724304\n",
            "Epoch: 456\n",
            "Train loss: 0.023463739082217216\n",
            "Val loss: 0.5652399659156799\n",
            "Epoch: 457\n",
            "Train loss: 0.022648418322205544\n",
            "Val loss: 0.5596599578857422\n",
            "Epoch: 458\n",
            "Train loss: 0.02423633448779583\n",
            "Val loss: 0.5661705732345581\n",
            "Epoch: 459\n",
            "Train loss: 0.02764798514544964\n",
            "Val loss: 0.5758258104324341\n",
            "Epoch: 460\n",
            "Train loss: 0.02664553001523018\n",
            "Val loss: 0.5659357905387878\n",
            "Epoch: 461\n",
            "Train loss: 0.047349292784929276\n",
            "Val loss: 0.5747589468955994\n",
            "Epoch: 462\n",
            "Train loss: 0.038309939205646515\n",
            "Val loss: 0.5440438985824585\n",
            "Epoch: 463\n",
            "Train loss: 0.030104881152510643\n",
            "Val loss: 0.5561831593513489\n",
            "Epoch: 464\n",
            "Train loss: 0.017615964636206627\n",
            "Val loss: 0.5813735723495483\n",
            "Epoch: 465\n",
            "Train loss: 0.013308505527675152\n",
            "Val loss: 0.5766397714614868\n",
            "Epoch: 466\n",
            "Train loss: 0.0195626188069582\n",
            "Val loss: 0.5550114512443542\n",
            "Epoch: 467\n",
            "Train loss: 0.04561268910765648\n",
            "Val loss: 0.5607960224151611\n",
            "Epoch: 468\n",
            "Train loss: 0.02734481729567051\n",
            "Val loss: 0.555612325668335\n",
            "Epoch: 469\n",
            "Train loss: 0.019438939169049263\n",
            "Val loss: 0.5655412673950195\n",
            "Epoch: 470\n",
            "Train loss: 0.02233070880174637\n",
            "Val loss: 0.565814733505249\n",
            "Epoch: 471\n",
            "Train loss: 0.027902036905288696\n",
            "Val loss: 0.569986879825592\n",
            "Epoch: 472\n",
            "Train loss: 0.03814256936311722\n",
            "Val loss: 0.579736590385437\n",
            "Epoch: 473\n",
            "Train loss: 0.02116035483777523\n",
            "Val loss: 0.5790358781814575\n",
            "Epoch: 474\n",
            "Train loss: 0.014346599578857422\n",
            "Val loss: 0.5932016372680664\n",
            "Epoch: 475\n",
            "Train loss: 0.018433827906847\n",
            "Val loss: 0.5949735641479492\n",
            "Epoch: 476\n",
            "Train loss: 0.01474569458514452\n",
            "Val loss: 0.5708645582199097\n",
            "Epoch: 477\n",
            "Train loss: 0.030873361974954605\n",
            "Val loss: 0.5543924570083618\n",
            "Epoch: 478\n",
            "Train loss: 0.02329484187066555\n",
            "Val loss: 0.5430722832679749\n",
            "Epoch: 479\n",
            "Train loss: 0.01893865503370762\n",
            "Val loss: 0.5325326919555664\n",
            "Epoch: 480\n",
            "Train loss: 0.020848101004958153\n",
            "Val loss: 0.5289677977561951\n",
            "Epoch: 481\n",
            "Train loss: 0.022764425724744797\n",
            "Val loss: 0.5333171486854553\n",
            "Epoch: 482\n",
            "Train loss: 0.034742847084999084\n",
            "Val loss: 0.5711617469787598\n",
            "Epoch: 483\n",
            "Train loss: 0.033323582261800766\n",
            "Val loss: 0.5781262516975403\n",
            "Epoch: 484\n",
            "Train loss: 0.02847854234278202\n",
            "Val loss: 0.5699470639228821\n",
            "Epoch: 485\n",
            "Train loss: 0.01822102814912796\n",
            "Val loss: 0.5585770010948181\n",
            "Epoch: 486\n",
            "Train loss: 0.02436661534011364\n",
            "Val loss: 0.5673108696937561\n",
            "Epoch: 487\n",
            "Train loss: 0.028905360028147697\n",
            "Val loss: 0.5958108901977539\n",
            "Epoch: 488\n",
            "Train loss: 0.014930865727365017\n",
            "Val loss: 0.5866232514381409\n",
            "Epoch: 489\n",
            "Train loss: 0.014013341628015041\n",
            "Val loss: 0.5712436437606812\n",
            "Epoch: 490\n",
            "Train loss: 0.029900718480348587\n",
            "Val loss: 0.5672916769981384\n",
            "Epoch: 491\n",
            "Train loss: 0.019028756767511368\n",
            "Val loss: 0.5717658400535583\n",
            "Epoch: 492\n",
            "Train loss: 0.012338303029537201\n",
            "Val loss: 0.5929898023605347\n",
            "Epoch: 493\n",
            "Train loss: 0.015193327330052853\n",
            "Val loss: 0.5848656892776489\n",
            "Epoch: 494\n",
            "Train loss: 0.03090497851371765\n",
            "Val loss: 0.582511305809021\n",
            "Epoch: 495\n",
            "Train loss: 0.011379200033843517\n",
            "Val loss: 0.597909152507782\n",
            "Epoch: 496\n",
            "Train loss: 0.01855314150452614\n",
            "Val loss: 0.5884645581245422\n",
            "Epoch: 497\n",
            "Train loss: 0.026675058528780937\n",
            "Val loss: 0.5720848441123962\n",
            "Epoch: 498\n",
            "Train loss: 0.03354945778846741\n",
            "Val loss: 0.5688781142234802\n",
            "Epoch: 499\n",
            "Train loss: 0.014056679792702198\n",
            "Val loss: 0.5795251727104187\n"
          ]
        }
      ],
      "source": [
        "model.to('cuda')\n",
        "for i in range(300,500):\n",
        "    print('Epoch:', i)\n",
        "    loss_train =  train_one_epoch(model, optimizer, train_loader)\n",
        "    loss_val =  val_loss(model, val_loader)\n",
        "    writer.add_scalar('Loss/train', loss_train, i)\n",
        "    writer.add_scalar('Loss/val', loss_val, i)\n",
        "    print('Train loss:', loss_train)\n",
        "    print('Val loss:', loss_val)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VbnVCnCCUCIL"
      },
      "outputs": [],
      "source": [
        "torch.save({\"model\":model.to('cpu'), \"parameters\":model.to('cpu').state_dict()}, 'model500.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI7uAwVmUCIL"
      },
      "outputs": [],
      "source": [
        "# model.load_state_dict(torch.load('model.pth')['parameters'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkZKRodQUCIL",
        "outputId": "61a3f154-8b78-4cef-b57d-1c697df3b49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recommendation accuracy: 0.8783333333333333\n",
            "Val Recommendation accuracy: 0.55\n",
            "Top 2 accuracy: 0.9566666666666667\n",
            "Val Top 2 accuracy: 0.8\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def recom_accuracy(x,y_recom):\n",
        "  recom = F.softmax(model(x), dim=1)\n",
        "  recom = torch.argmax(recom, dim=1)\n",
        "  y_recom = torch.argmax(y_recom, dim=1)\n",
        "  return torch.sum(recom == y_recom).item() / len(y_recom)\n",
        "#Caclulate the accuracy of all datasets\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "x_all = torch.tensor(data[data.columns[5:]].values, dtype=torch.float32).to('cuda')\n",
        "y_recommend_all = torch.tensor(data[data.columns[:5]].values, dtype=torch.float32).to('cuda')\n",
        "acc_sum = 0\n",
        "for i in range(len(x_all)//32 + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    x = x_all[i*32:(i+1)*32]\n",
        "    y_recommend = y_recommend_all[i*32:(i+1)*32]\n",
        "    acc_sum += recom_accuracy(x, y_recommend)*x.shape[0]\n",
        "print('Recommendation accuracy:', acc_sum/len(x_all))\n",
        "\n",
        "#Calulate Val accuracy\n",
        "torch.cuda.empty_cache()\n",
        "for x_val, y_recommend_val in val_loader:\n",
        "    x_val = x_val.to('cuda')\n",
        "    y_recommend_val = y_recommend_val.to('cuda')\n",
        "acc_sum_val = 0\n",
        "for i in range(len(x_val)//32 + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    x = x_val[i*32:(i+1)*32]\n",
        "    y_recommend = y_recommend_val[i*32:(i+1)*32]\n",
        "    acc_sum_val += recom_accuracy(x, y_recommend)*x.shape[0]\n",
        "print('Val Recommendation accuracy:', acc_sum_val/len(x_val))\n",
        "\n",
        "#Calculate the top 2 accuracy\n",
        "@torch.no_grad()\n",
        "def top2_accuracy(x, y_recom):\n",
        "    recom = F.softmax(model(x), dim=1)\n",
        "    _, top2_indices = torch.topk(recom, k=2, dim=1)\n",
        "    y_recom = torch.argmax(y_recom, dim=1)\n",
        "\n",
        "    correct = 0\n",
        "    for i in range(len(y_recom)):\n",
        "        if y_recom[i] in top2_indices[i]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(y_recom)\n",
        "acc_sum = 0\n",
        "for i in range(len(x_all)//32 + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    x = x_all[i*32:(i+1)*32]\n",
        "    y_recommend = y_recommend_all[i*32:(i+1)*32]\n",
        "    acc_sum += top2_accuracy(x, y_recommend)*x.shape[0] \n",
        "print('Top 2 accuracy:', acc_sum/len(x_all))\n",
        "\n",
        "#Calculate the top 2 accuracy of val\n",
        "torch.cuda.empty_cache()\n",
        "for x_val, y_recommend_val in val_loader:\n",
        "    x_val = x_val.to('cuda')\n",
        "    y_recommend_val = y_recommend_val.to('cuda')\n",
        "acc_sum_val = 0\n",
        "for i in range(len(x_val)//32 + 1):\n",
        "    torch.cuda.empty_cache()\n",
        "    x = x_val[i*32:(i+1)*32]\n",
        "    y_recommend = y_recommend_val[i*32:(i+1)*32]\n",
        "    acc_sum_val += top2_accuracy(x, y_recommend)*x.shape[0]\n",
        "print('Val Top 2 accuracy:', acc_sum_val/len(x_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "suVAOxOrUCIL",
        "outputId": "657ddbed-f2a1-4202-b967-96180e00d96a"
      },
      "outputs": [],
      "source": [
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
